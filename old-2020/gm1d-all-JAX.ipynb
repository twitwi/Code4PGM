{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import obj_dic, show_heatmap, show_heatmap_contours\n",
    "\n",
    "import jax\n",
    "from jax import numpy as jnp, random\n",
    "import numpy as _np # mostly for plotting\n",
    "from scipy.stats import norm, invgamma, beta # mostly for plotting\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "md = lambda *args: display(Markdown(*args))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "r = random.PRNGKey(123456)\n",
    "r, k = random.split(r, 2)\n",
    "\n",
    "# Generate the Dataset as N draws from a normal distribution with base_mean and base_stdev\n",
    "\n",
    "base_mean = 4\n",
    "base_stdev = 1\n",
    "N = 100\n",
    "X = random.normal(k, (N, 1)) * base_stdev + base_mean\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "plt.scatter([],[])\n",
    "plt.scatter(X[:,0], _np.random.uniform(-1, 1, (N)))\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(str(N)+\" 1d observations (y-axis is random)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute (on a grid, up to a constant factor) and draw the true posterior p(θ|X)\n",
    "\n",
    "def plot_true_distribution(contours=True, cmap=\"gray\",\n",
    "                           minx=base_mean-0.3,\n",
    "                           maxx=base_mean+0.3,\n",
    "                           miny=base_stdev/1.3,\n",
    "                           maxy=base_stdev*1.3,\n",
    "                           prior=None):\n",
    "    mm = lambda a: (_np.min(a), _np.max(a))\n",
    "    Lprop = lambda μ, σ, X: 1/(2*_np.pi*σ**2)**(X.shape[0]/2) * _np.exp(- _np.sum((X - μ)**2, axis=0, keepdims=True) / (2*σ**2))\n",
    "    linspaceμ = _np.linspace(minx, maxx, 101)\n",
    "    linspaceσ = _np.linspace(miny, maxy, 103)\n",
    "    Lmap = Lprop(linspaceμ[None,:,None], linspaceσ[None,None,:], X[:,None])\n",
    "    if prior is not None:\n",
    "        Lmap *= prior(linspaceμ[:,None], linspaceσ[None,:])\n",
    "    if contours:\n",
    "        plt.contour((Lmap[0].T), extent=[*mm(linspaceμ), *mm(linspaceσ)], origin='lower', cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow((Lmap[0].T), extent=[*mm(linspaceμ), *mm(linspaceσ)], origin='lower', aspect='auto')\n",
    "        plt.colorbar()\n",
    "    return obj_dic(locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the true posterior\n",
    "\n",
    "default_plot = plot_true_distribution(False)\n",
    "plot_true_distribution()\n",
    "plt.show()\n",
    "plot_true_distribution()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "md(\"# GIBBS SAMPLING EXAMPLE\")\n",
    "\n",
    "def do_gibbs_uninformative_prior(r, X = jnp.array(X)):\n",
    "    μ = 5  ####### GIBBS: initialize\n",
    "    σ = 6  ####### GIBBS: initialize\n",
    "    \n",
    "    μs = [μ]\n",
    "    σs = [σ]\n",
    "\n",
    "    ITER = 20000//10\n",
    "    BURN = 100\n",
    "    PLOT_AT = [200 // 2]\n",
    "    PLOT_NICE_AT = [0, 1, 2, 3, 4]\n",
    "    md('### Running the Gibbs sampler, showing the successive conditional probabilities')\n",
    "    for i in range(ITER//2):  ####### GIBBS: loop\n",
    "        \n",
    "        # scipy normal distribution uses the standard deviation\n",
    "        r, k = random.split(r, 2)\n",
    "        μ = random.normal(k)*σ/N**0.5 + jnp.mean(X)  ####### GIBBS: one step\n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "\n",
    "        r, k = random.split(r, 2)\n",
    "        invσ = random.gamma(k, N/2 - 1) / (jnp.sum((X-μ)**2)/2)\n",
    "        σ = 1 / invσ ** 0.5 ####### GIBBS: another step\n",
    "        #σ = invgamma.rvs(N/2 - 1, 0, np.sum((X-μ)**2)/2) ** 0.5  \n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "        \n",
    "        if i in PLOT_NICE_AT:\n",
    "            minx = min(base_mean-0.3, _np.min(μs))\n",
    "            maxx = max(base_mean+0.3, _np.max(μs))\n",
    "            miny = min(base_stdev/1.3, _np.min(σs))\n",
    "            maxy = max(base_stdev*1.3, _np.max(σs))\n",
    "            ax = plt.subplot(3, 3, (4,8))\n",
    "            plt.plot(μs, σs, alpha=0.6)\n",
    "            plt.scatter(μs, σs, marker='x')\n",
    "            \n",
    "            plt.subplot(3, 3, (1,2), sharex=ax)\n",
    "            x = _np.linspace(minx, maxx, 151)\n",
    "            plt.plot(x, jax.scipy.stats.norm.pdf(x, _np.mean(X), σs[-3] / N**0.5))\n",
    "\n",
    "            plt.subplot(3, 3, (6,9), sharey=ax)\n",
    "            x = _np.linspace(miny, maxy, 151)\n",
    "            #plt.plot(invgamma.pdf(x**2, N/2 - 1, 0, _np.sum((X-μs[-2])**2)/2), x)\n",
    "\n",
    "            ax.scatter(μs[-3:], σs[-3:], c='r')\n",
    "            plt.show()\n",
    "        if i in PLOT_AT:\n",
    "            md('### Plotting after '+str(i)+' samples')\n",
    "            plt.plot(μs, σs, alpha=0.2)\n",
    "            plt.scatter(μs, σs, marker='.')\n",
    "            plt.title('All samples')\n",
    "            plt.show()\n",
    "\n",
    "            plt.scatter(μs[BURN:], σs[BURN:], marker='x')\n",
    "            plt.xlim(_np.min(μs), _np.max(μs))\n",
    "            plt.ylim(_np.min(σs), _np.max(σs))\n",
    "            plt.title('Samples after burn-in period')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.scatter(μs[BURN:], σs[BURN:], marker='x')\n",
    "            plt.title('Samples after burn-in period (zoomed)')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    md('Estimating the p(θ|X) as an histogram from samples (true posterior with lines)')\n",
    "    show_heatmap(μs[BURN:], σs[BURN:], bins=25)\n",
    "    plot_true_distribution()\n",
    "    plt.show()\n",
    "    return obj_dic(locals())\n",
    "\n",
    "gibbs = do_gibbs_uninformative_prior(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "md(\"# GIBBS SAMPLING EXAMPLE (plot at end only)\")\n",
    "\n",
    "def do_gibbs_uninformative_prior_simpler(r, X = jnp.array(X)):\n",
    "    μ = 5  ####### GIBBS: initialize\n",
    "    σ = 6  ####### GIBBS: initialize\n",
    "    \n",
    "    μs = [μ]\n",
    "    σs = [σ]\n",
    "\n",
    "    ITER = 20000//10\n",
    "    BURN = 100\n",
    "    md('### Running the Gibbs sampler')\n",
    "    for i in range(ITER//2):  ####### GIBBS: loop\n",
    "        \n",
    "        # scipy normal distribution uses the standard deviation\n",
    "        r, k = random.split(r, 2)\n",
    "        μ = random.normal(k)*σ/N**0.5 + jnp.mean(X)  ####### GIBBS: one step\n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "\n",
    "        r, k = random.split(r, 2)\n",
    "        invσ = random.gamma(k, N/2 - 1) / (jnp.sum((X-μ)**2)/2)\n",
    "        σ = 1 / invσ ** 0.5 ####### GIBBS: another step\n",
    "        #σ = invgamma.rvs(N/2 - 1, 0, np.sum((X-μ)**2)/2) ** 0.5  \n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "\n",
    "    md('Estimating the p(θ|X) as an histogram from samples (true posterior with lines)')\n",
    "    show_heatmap(μs[BURN:], σs[BURN:], bins=25)\n",
    "    plot_true_distribution()\n",
    "    plt.show()\n",
    "    return obj_dic(locals())\n",
    "\n",
    "gibbs = do_gibbs_uninformative_prior_simpler(r)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "## NB: much more iterations, way faster\n",
    "## NB: with 20000*100, the numpy version takes around 88sec\n",
    "## NB: with 20000*1 , the bad jax version takes 17sec so 1700sec in comparison\n",
    "## NB: on the same machine, no GPU, this jax.lax.scan version takes around 3.8sec (20× speedup)\n",
    "\n",
    "md(\"# GIBBS SAMPLING EXAMPLE (fast with a lax.scan)\")\n",
    "\n",
    "def do_gibbs_uninformative_prior_scan(r, X = jnp.array(X)):\n",
    "    μ = 5  ####### GIBBS: initialize\n",
    "    σ = 6  ####### GIBBS: initialize\n",
    "        \n",
    "    def scanner(μ_σ_r, i):\n",
    "        μ, σ, r = μ_σ_r\n",
    "        r, *k = random.split(r, 3)\n",
    "        μ1 = random.normal(k[0])*σ/N**0.5 + jnp.mean(X)  ####### GIBBS: one step\n",
    "        invσ2 = random.gamma(k[1], N/2 - 1) / (jnp.sum((X-μ1)**2)/2)\n",
    "        σ2 = 1 / invσ2 ** 0.5 ####### GIBBS: another step\n",
    "        return (μ1, σ2, r), [μ1, μ1, σ, σ2]\n",
    "\n",
    "\n",
    "    ITER = 20000*100\n",
    "    BURN = 100\n",
    "    \n",
    "    md('### Running the Gibbs sampler')\n",
    "    μ_σ_r, history = jax.lax.scan(scanner, (μ, σ, r), jnp.arange(ITER))\n",
    "    print(\"OK\", μ_σ_r)\n",
    "    print(\"H\", history)\n",
    "    \n",
    "    μs = [v for l in history[:2] for v in l]\n",
    "    σs = [v for l in history[2:] for v in l] \n",
    "\n",
    "    print('Estimating the p(θ|X) as an histogram from samples (true posterior with lines)')\n",
    "    show_heatmap(μs[BURN:], σs[BURN:], bins=25)\n",
    "    plot_true_distribution()\n",
    "    plt.show()\n",
    "    return obj_dic(locals())\n",
    "\n",
    "gibbs = do_gibbs_uninformative_prior_scan(r)\n",
    "\n",
    "## TODO show trajectories (also in the numpy version)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "md('# METROPOLIS-HASTINGS EXAMPLE')\n",
    "\n",
    "def do_metropolis_hastings(prior=False, show=True, ITER=20000*1, INIT=(10,0.1), md=md):\n",
    "    μ, σ = INIT ####### METROPOLIS-HASTINGS: initialize\n",
    "\n",
    "    xsh0 = X.shape[0]\n",
    "    lnLproportional = jax.jit(lambda μ, σ: -(xsh0*jnp.log(σ) + jnp.sum((X - μ)**2)/2/σ**2)) # up to log((2π)^½N)\n",
    "        \n",
    "    if prior == 'Funky':\n",
    "        lnprior = jax.jit(lambda μ, σ: 100*jnp.log(jnp.clip(2.5 - σ, 0.0001, 1)))\n",
    "    elif prior:\n",
    "        lnprior = lambda μ, σ: -(μ-5)**2/.2 + -(σ-3)**2/.2  # kind of gaussian prior around μ=5, σ=2, pretty tight, so we see that even with 100 observations the prior impacts significantly what is the optimal p(μ,σ|X)\n",
    "    else:\n",
    "        lnprior = lambda *a:0 # uninformative prior, or very very broad/smooth\n",
    "        \n",
    "    BURN = 1000\n",
    "    μσvar = .1\n",
    "\n",
    "    def scanner(μ_σ_r, i):\n",
    "        μ, σ, r = μ_σ_r\n",
    "        r, *k = random.split(r, 4)\n",
    "        \n",
    "        μnew = random.normal(k[0])*μσvar + μ\n",
    "        σnew = random.normal(k[1])*μσvar + σ\n",
    "        \n",
    "        paccept = jnp.exp(\n",
    "            lnLproportional(μnew, σnew) - lnLproportional(μ, σ)\n",
    "            + lnprior(μnew, σnew) - lnprior(μ, σ) # prior\n",
    "        ) ####### METROPOLIS-HASTINGS: compute acceptance \"probability\" (might be > 1)\n",
    "        \n",
    "        a = random.uniform(k[2]) < paccept\n",
    "        μ = jnp.where(a, μnew, μ)\n",
    "        σ = jnp.where(a, σnew, σ)\n",
    "        return (μ, σ, r), [μ, σ, μnew, σnew, a]\n",
    "    \n",
    "    \n",
    "    md('... Running the MH sampler')\n",
    "    μ_σ_r, history = jax.lax.scan(scanner, (μ, σ, r), jnp.arange(ITER))\n",
    "    #print(\"OK\", μ_σ_r)\n",
    "    #print(\"H\", history)\n",
    "\n",
    "    history = _np.array(history)\n",
    "    μs = history[0]\n",
    "    σs = history[1]\n",
    "    rej_μs = [history[2,i] for i in range(history.shape[1]) if not history[4,i]]\n",
    "    rej_σs = [history[3,i] for i in range(history.shape[1]) if not history[4,i]]\n",
    "\n",
    "    plt.plot(μs, σs, alpha=0.6)\n",
    "    if show:\n",
    "        plt.scatter(μs[:400], σs[:400], marker='x')\n",
    "        plt.scatter(history[2, :400][history[4, :400]==False], history[3, :400][history[4, :400]==False], marker='x', c='r')\n",
    "        show_heatmap_contours(norm.rvs(μs[-1], μσvar, 10000), norm.rvs(σs[-1], μσvar, 10000))\n",
    "        plt.show()\n",
    "\n",
    "    if show:\n",
    "        show_heatmap(μs[BURN:], σs[BURN:], bins=25)\n",
    "        plot_true_distribution()\n",
    "        plt.show()\n",
    "    md(\"Accepted \" + str(len(μs)-len(rej_μs)) + \" and rejected \" + str(len(rej_μs)))\n",
    "    return obj_dic(locals())\n",
    "\n",
    "\n",
    "md(\"## Showing accepted samples and rejected samples (1 chain)\")\n",
    "md(\"### No prior\")\n",
    "mh = do_metropolis_hastings()\n",
    "md(\"### Prior that biases the estimate\")\n",
    "mh = do_metropolis_hastings(True)\n",
    "md(\"### Prior that forbids σ > 2.5\")\n",
    "mh = do_metropolis_hastings(\"Funky\")\n",
    "\n",
    "\n",
    "md(\"## Draw a few chains\")\n",
    "for setup in [False, True, 'Funky']:\n",
    "    md('Prior: '+str(setup))\n",
    "    for i in range(10):\n",
    "        do_metropolis_hastings(setup, show=False, ITER=500, INIT=(norm.rvs(10, 2), _np.abs(norm.rvs(1, 2))), md=lambda *a:0)\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "\n",
    "md('# VARIATIONAL INFERENCE EXAMPLE')\n",
    "\n",
    "def do_variational_inference(PRIOR=(5,1 , 1,1.5**2), ITER=1000):\n",
    "    # we clearly don't need 1000 iterations but they are so cheap... (no sampling => very fast)\n",
    "    \n",
    "    µ0, λ0, α0, β0 = PRIOR ####### VARIATIONAL INFERENCE: prior\n",
    "    \n",
    "    ####### VARIATIONAL INFERENCE:  set inital values of the variational parameters using the prior\n",
    "    μN = µ0\n",
    "    σN = (β0/α0/λ0)**0.5\n",
    "    αN = α0\n",
    "    βN = β0\n",
    "    toADD = (μN, σN, αN, βN)\n",
    "\n",
    "    PLOT_AT = [ITER-1]\n",
    "    PLOT_PDF_AT = [0, 1, 2, 3, 4, ITER-1]\n",
    "    PLOT_BROAD =  [0, 1, 2, 3]\n",
    "\n",
    "    ####### VARIATIONAL INFERENCE: constants\n",
    "    Xsum = jnp.sum(X)\n",
    "    X2sum = jnp.sum(X**2)\n",
    "\n",
    "    def scanner(varpar, i):\n",
    "        μN, σN, αN, βN = varpar\n",
    "        αN,βN,μN,σN = (\n",
    "            α0 + (N+1)/2,\n",
    "            β0 + 0.5 * ( (λ0+N)*(σN**2+μN**2) - 2*(μ0*λ0+Xsum)*μN + X2sum + λ0*μ0**2 ),\n",
    "            (Xsum + λ0*μ0) / (N + λ0),\n",
    "            (βN/αN/(N+λ0))**0.5,\n",
    "        )\n",
    "        return (μN, σN, αN, βN), [μN, σN, αN, βN]\n",
    "    \n",
    "    \n",
    "    md('... Running the VI loop')\n",
    "    (μN, σN, αN, βN), history = jax.lax.scan(scanner, (μN, σN, αN, βN), jnp.arange(ITER))\n",
    "    #print(\"OK\", μ_σ_r)\n",
    "    #print(\"H\", history)\n",
    "\n",
    "    history = _np.concatenate((_np.array([toADD]), _np.array(history).T), axis=0)\n",
    "\n",
    "    for i in sorted(list(set(PLOT_AT + PLOT_PDF_AT))):\n",
    "        μN,σN,αN,βN = history[i]\n",
    "        if i in PLOT_PDF_AT:\n",
    "            md('Current variational estimation q(θ) (heatmap) vs true posterior p(θ|X) (isolines) (and also without prior)')\n",
    "            linspaceμ = default_plot.linspaceμ\n",
    "            linspaceσ = default_plot.linspaceσ\n",
    "            if i in PLOT_BROAD:\n",
    "                linspaceμ = _np.linspace(0, 6, 303)\n",
    "                linspaceσ = _np.linspace(0.01, 2, 301)\n",
    "            pσ = invgamma.pdf(linspaceσ**2, αN, 0, βN)\n",
    "            pμ = norm.pdf(linspaceμ[:,None], μN, σN)\n",
    "            pμσ = pμ*pσ[None,:]\n",
    "\n",
    "            mm = default_plot.mm\n",
    "            plt.imshow(pμσ.T, extent=[*mm(linspaceμ), *mm(linspaceσ)], origin='lower', aspect='auto')\n",
    "            #plt.colorbar()\n",
    "            plot_true_distribution()\n",
    "            plot_true_distribution(prior=lambda μ,σ: invgamma.pdf(σ**2, α0, 0, β0) * norm.pdf(μ, μ0, σ/λ0**0.5))\n",
    "            plt.title(\"$q(\\\\mu, \\\\sigma)^{(color)}$ vs $p(\\\\mu, \\\\sigma | X)^{(isolines)}$, after \"+str(i+1)+\" iter.\")\n",
    "            plt.show()\n",
    " \n",
    "        if i in PLOT_AT:\n",
    "            md('Plotting the mean of the estimate accross iterations (we see a convergence)')\n",
    "            h = _np.array(history)\n",
    "            μs = h[:,0]\n",
    "            σs = (h[:,3]/h[:,2])**0.5\n",
    "            plt.title(\"successive positions of the mean of $q(\\\\mu, \\\\sigma)$\")\n",
    "            plt.plot(μs, σs, alpha=0.6)\n",
    "            plt.scatter(μs, σs, marker='x')\n",
    "            plt.show()\n",
    "\n",
    "    return obj_dic(locals())\n",
    "\n",
    "md('## Not too bad prior and initialization')\n",
    "vi = do_variational_inference()\n",
    "md('## Poor prior and initialization')\n",
    "# 42 virtual points at value 5, \n",
    "vi = do_variational_inference(PRIOR=(5,42 , 1,1.5**2), ITER=10)\n",
    "md('## Fuzzy prior and initialization')\n",
    "# 5 points at 5\n",
    "vi = do_variational_inference(PRIOR=(5,5 , 1,1.5**2), ITER=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
