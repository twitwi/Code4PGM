{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e4b2d9-73c3-43b5-b971-48281ab21a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "import arviz as az\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.utils.data import DataLoader, TensorDataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2335cd10-faf3-464f-9798-496599c4ab1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import obj_dic, show_heatmap_contours, show_heatmap\n",
    "\n",
    "SEED = 1412\n",
    "\n",
    "def gen_data(N, seed=SEED):\n",
    "    r = np.random.default_rng(seed)\n",
    "    u = r.uniform(0, 1, (N,))\n",
    "    u2 = u[...,None]\n",
    "    m = np.array([-10, -5]) + 20*u2\n",
    "    m -= 10*np.array([0, 1])*(2*u2-1)**2\n",
    "    m += 10*np.array([0, 1])*np.sin(u2*10)\n",
    "    p = m + r.normal(0, .1, (N, 2))\n",
    "    return p, obj_dic(locals())\n",
    "\n",
    "data, gt = gen_data(2500)\n",
    "print(data.shape)\n",
    "\n",
    "plt.scatter(data[:,0], data[:,1], marker='.', alpha=0.1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf742986-ceec-434d-9e1e-c70beaca9b23",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AE1, self).__init__()\n",
    "        self.linear_relu_stack = nn.Sequential(\n",
    "            nn.Linear(2, 1),\n",
    "            nn.Linear(1, 2),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        logits = self.linear_relu_stack(x)\n",
    "        return logits\n",
    "\n",
    "m1 = AE1()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65a41ae8-b3d2-4775-831b-a84ecf6f516f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# untrained AE network\n",
    "print(data[0,:])\n",
    "m1.forward(torch.Tensor(data[0,:]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd39cd0-6f84-4ce0-9577-3310804d701b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_model(m):\n",
    "    plt.scatter(data[:,0], data[:,1], marker='.', alpha=0.1)\n",
    "    recons = m(torch.Tensor(data)).detach().numpy()\n",
    "    plt.scatter(recons[:,0], recons[:,1], marker='.', alpha=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69eaaae2-b86f-4907-b610-915974d2020c",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79bdb480-2bee-4e44-a987-34beb884a292",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = DataLoader(\n",
    "    TensorDataset(torch.Tensor(data),\n",
    "                  torch.Tensor(data)),\n",
    "    batch_size=128)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "668e4ab1-a772-4b94-a3cf-787f41f171ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop(dataloader, model, loss_fn, optimizer):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        loss = loss_fn(pred, y)\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch+1) % 100 == 0:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f}  [{current:>5d}/{size:>5d}]\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "607797c7-073f-4a5f-92d3-ab7c4f9517c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, epochs = 100):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        #print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop(train_dataloader, model, loss_fn, optimizer)\n",
    "        #test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24b3d862-e6dc-4afd-b741-9d86762b4547",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a73e1ef-e866-4249-a667-a32be44df7b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(m1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d23fcd4e-3e0e-42a1-9e3d-ef2cd4021da0",
   "metadata": {},
   "outputs": [],
   "source": [
    "class AE2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super(AE2, self).__init__()\n",
    "        D = 2 # dim of the X space\n",
    "        L = 1 # dim of the latent space\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(D, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, L),\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(L, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, D),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        pred = self.decoder(self.encoder(x))\n",
    "        return pred\n",
    "\n",
    "m2 = AE2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7135179-833a-4f6d-9509-5c3e260d631d",
   "metadata": {},
   "outputs": [],
   "source": [
    "train(m2, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d08f10b3-098a-4c1b-adef-cae3224ef0ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0849c914-2d48-4061-b55a-e7b187e497f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_latent_hist(m):\n",
    "    plt.hist(m.encoder(torch.Tensor(data))[:,0].detach().numpy(), bins=100);\n",
    "\n",
    "plot_latent_hist(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20255e74-baa4-4380-9bcd-ea2d7926efdc",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VAE2(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        D = 2 # dim of the X space\n",
    "        L = 1 # dim of the latent space\n",
    "        super(VAE2, self).__init__()\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(D, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100,    L+L   ), # a mean on z, and a logvar on z (so that exp(logvar) is always positive\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(1, 100),\n",
    "            nn.ReLU(),\n",
    "            nn.Linear(100, D),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        D = 2 # dim of the X space\n",
    "        L = 1 # dim of the latent space\n",
    "        mu_logvar = self.encoder(x)\n",
    "        \n",
    "        mu = mu_logvar[:,0:1]\n",
    "        std = torch.exp(mu_logvar[:,1:2]/2)\n",
    "\n",
    "        z = mu + std * torch.normal(0, 1, (x.shape[0], L))\n",
    "        pred = self.decoder(z)\n",
    "        return pred\n",
    "\n",
    "vm2 = VAE2()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a39ea184-2fd5-42ef-85b3-c324cc942151",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_loop_with_KL(dataloader, model, loss_fn, optimizer, showloss):\n",
    "    size = len(dataloader.dataset)\n",
    "    for batch, (X, y) in enumerate(dataloader):\n",
    "        # Compute prediction and loss\n",
    "        pred = model(X)\n",
    "        mu_logvar = model.encoder(X)\n",
    "        var = torch.exp(mu_logvar[:,1])\n",
    "        kl = 0.5 * (mu_logvar[:,0]**2 + var - torch.log(var))\n",
    "        loss = loss_fn(pred, y) + kl.mean()\n",
    "\n",
    "        # Backpropagation\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        if (batch) % 100 == 0 and showloss:\n",
    "            loss, current = loss.item(), batch * len(X)\n",
    "            print(f\"loss: {loss:>7f} {kl.mean():>7f}  [{current:>5d}/{size:>5d}]\")\n",
    "\n",
    "def train_with_KL(model, epochs = 100, showloss=False, lr=2e-2):\n",
    "    loss_fn = nn.MSELoss()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "\n",
    "    for t in range(epochs):\n",
    "        #print(f\"Epoch {t+1}\\n-------------------------------\")\n",
    "        train_loop_with_KL(train_dataloader, model, loss_fn, optimizer, showloss)\n",
    "        #test_loop(test_dataloader, model, loss_fn)\n",
    "    print(\"Done!\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0349ea-d33e-4adc-a882-deffcfb21475",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_KL(vm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1272db19-b13d-4833-b233-79b13354ef1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(vm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "605061d7-6042-42b9-b822-207910258c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_hist(vm2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72b5b0e4-1ba1-4e09-a619-5414603eaf8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Unfruitful attempt to improve this simple case\n",
    "\n",
    "\n",
    "class ResnetLinear(nn.Module):\n",
    "\n",
    "    def __init__(self, INOUT, MID):\n",
    "        super(ResnetLinear, self).__init__()\n",
    "        self.residual = nn.Sequential(\n",
    "            nn.Linear(INOUT, MID),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(MID, INOUT)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return x + self.residual(x)\n",
    "\n",
    "class VAE3(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        D = 2 # dim of the X space\n",
    "        L = 1 # dim of the latent space\n",
    "        super(VAE3, self).__init__()\n",
    "        H = 10\n",
    "        self.encoder = nn.Sequential(\n",
    "            nn.Linear(D, H),\n",
    "            ResnetLinear(H, H),\n",
    "            ResnetLinear(H, H),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(H, L+L)\n",
    "        )\n",
    "        self.decoder = nn.Sequential(\n",
    "            nn.Linear(L, H),\n",
    "            ResnetLinear(H, H),\n",
    "            ResnetLinear(H, H),\n",
    "            nn.SiLU(),\n",
    "            nn.Linear(H, D)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        D = 2 # dim of the X space\n",
    "        L = 1 # dim of the latent space\n",
    "        mu_logvar = self.encoder(x)\n",
    "        \n",
    "        mu = mu_logvar[:,0:1]\n",
    "        std = torch.exp(mu_logvar[:,1:2]/2)\n",
    "\n",
    "        z = mu + torch.normal(0, 1, (x.shape[0], L))\n",
    "        pred = self.decoder(z)\n",
    "        return pred\n",
    "\n",
    "def init_weights(m):\n",
    "    if isinstance(m, nn.Linear):\n",
    "        torch.nn.init.xavier_normal_(m.weight)\n",
    "        m.bias.data.fill_(0.01)\n",
    "        \n",
    "vm3 = VAE3()\n",
    "#vm3.encoder.apply(init_weights)\n",
    "#vm3.decoder.apply(init_weights)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43849541-b00d-4e3b-b494-9e9444c5ca3f",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_with_KL(vm3, epochs=100, showloss=True)\n",
    "#train(vm3, epochs=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b69368b-d86b-4b00-84d9-bdb3d8c2eac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_model(vm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "293d4346-ef43-46f8-8e2d-2f0111600c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_latent_hist(vm3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5adc96f6-5542-4ba6-9bb9-d34360593a7a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e9800a6-6c1f-495c-b347-0e71fdd873c7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
