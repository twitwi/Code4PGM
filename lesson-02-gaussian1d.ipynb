{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f40dfb13-0b58-41e4-ac8f-cb1533ba7425",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tools import obj_dic, show_heatmap, show_heatmap_contours\n",
    "import numpy as np\n",
    "\n",
    "from scipy.stats import norm, invgamma, beta\n",
    "import scipy.stats\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "from IPython.display import display, Markdown\n",
    "\n",
    "md = lambda *args: display(Markdown(*args))\n",
    "\n",
    "np.random.seed(12345)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afaaef60-d3d4-4e2d-ae1a-9b6bc62a7731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate the Dataset as N draws from a normal distribution with base_mean and base_stdev\n",
    "\n",
    "base_mean = 4\n",
    "base_stdev = 1\n",
    "N = 100\n",
    "#X = np.random.normal(40, 10, (N, 1))\n",
    "X = np.random.normal(base_mean, base_stdev, (N, 1))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce9c73e-0729-4718-898f-68c16e966431",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at the data\n",
    "plt.scatter([],[])\n",
    "plt.scatter(X[:,0], np.random.uniform(-1, 1, (N)))\n",
    "plt.ylim(-5, 5)\n",
    "plt.title(str(N)+\" 1d observations (y-axis is random)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b871b37a-4743-4aac-8448-9e3d94dbfcac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# define a function to compute (on a grid, up to a constant factor) and draw the true posterior p(θ|X)\n",
    "\n",
    "def plot_true_distribution(contours=True, cmap=\"gray\",\n",
    "                           minx=base_mean-0.3,\n",
    "                           maxx=base_mean+0.3,\n",
    "                           miny=base_stdev/1.3,\n",
    "                           maxy=base_stdev*1.3,\n",
    "                           prior=None):\n",
    "    mm = lambda a: (np.min(a), np.max(a))\n",
    "    Lprop = lambda μ, σ, X: 1/(2*np.pi*σ**2)**(X.shape[0]/2) * np.exp(- np.sum((X - μ)**2, axis=0, keepdims=True) / (2*σ**2))\n",
    "    linspaceμ = np.linspace(minx, maxx, 101)\n",
    "    linspaceσ = np.linspace(miny, maxy, 103)\n",
    "    Lmap = Lprop(linspaceμ[None,:,None], linspaceσ[None,None,:], X[:,None])\n",
    "    if prior is not None:\n",
    "        Lmap *= prior(linspaceμ[:,None], linspaceσ[None,:])\n",
    "    if contours:\n",
    "        plt.contour((Lmap[0].T), extent=[*mm(linspaceμ), *mm(linspaceσ)], origin='lower', cmap=cmap)\n",
    "    else:\n",
    "        plt.imshow((Lmap[0].T), extent=[*mm(linspaceμ), *mm(linspaceσ)], origin='lower', aspect='auto')\n",
    "        plt.colorbar()\n",
    "    return obj_dic(locals())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cb319cb-85aa-4687-8346-0feb2dbaa2b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show the true posterior\n",
    "\n",
    "default_plot = plot_true_distribution(False)\n",
    "plot_true_distribution()\n",
    "plt.show()\n",
    "plot_true_distribution()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69e70874-d5de-438a-8fd0-12732e3d2414",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "empirical_mean = X.mean()\n",
    "\n",
    "μ = 5\n",
    "σ = 6\n",
    "\n",
    "# θ^0\n",
    "μs = [μ]\n",
    "σs = [σ]\n",
    "\n",
    "for i in range(1000):\n",
    "    µ = np.random.normal(empirical_mean, σ/(N**0.5))\n",
    "    μs.append(μ)\n",
    "    σs.append(σ)\n",
    "\n",
    "    σ = scipy.stats.invgamma.rvs(N/2 - 1, 0, np.sum((X-μ)**2)/2) ** 0.5\n",
    "    μs.append(μ)\n",
    "    σs.append(σ)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a7a3759d-4363-4836-9e69-f286a40f6886",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(μs, σs)\n",
    "plot_true_distribution()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec0b3247-1bcc-4fee-af66-ced43bc001c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "md(\"# GIBBS SAMPLING EXAMPLE\")\n",
    "\n",
    "def do_gibbs_uninformative_prior():\n",
    "    μ = 5  ####### GIBBS: initialize\n",
    "    σ = 6  ####### GIBBS: initialize\n",
    "\n",
    "    μs = [μ]\n",
    "    σs = [σ]\n",
    "\n",
    "    ITER = 20000 #*100\n",
    "    BURN = 100\n",
    "    PLOT_AT = [200 // 2]\n",
    "    PLOT_NICE_AT = [0, 1, 2, 3, 4]\n",
    "    md('### Running the Gibbs sampler, showing the successive conditional probabilities')\n",
    "    for i in range(ITER//2):  ####### GIBBS: loop\n",
    "        \n",
    "        # scipy normal distribution uses the standard deviation\n",
    "        μ = norm.rvs(np.mean(X), σ / N**0.5)  ####### GIBBS: one step\n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "        \n",
    "        σ = invgamma.rvs(N/2 - 1, 0, np.sum((X-μ)** 2)/2) ** 0.5  ####### GIBBS: another step\n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "        \n",
    "        if i in PLOT_NICE_AT:\n",
    "            minx = min(base_mean-0.3, np.min(μs))\n",
    "            maxx = max(base_mean+0.3, np.max(μs))\n",
    "            miny = min(base_stdev/1.3, np.min(σs))\n",
    "            maxy = max(base_stdev*1.3, np.max(σs))\n",
    "            ax = plt.subplot(3, 3, (4,8))\n",
    "            plt.plot(μs, σs, alpha=0.6)\n",
    "            plt.scatter(μs, σs, marker='x')\n",
    "            \n",
    "            plt.subplot(3, 3, (1,2), sharex=ax)\n",
    "            x = np.linspace(minx, maxx, 151)\n",
    "            plt.plot(x, norm.pdf(x, np.mean(X), σs[-3] / N**0.5))\n",
    "\n",
    "            plt.subplot(3, 3, (6,9), sharey=ax)\n",
    "            x = np.linspace(miny, maxy, 151)\n",
    "            plt.plot(invgamma.pdf(x**2, N/2 - 1, 0, np.sum((X-μs[-2])**2)/2), x)\n",
    "\n",
    "            ax.scatter(μs[-3:], σs[-3:], c='r')\n",
    "            plt.show()\n",
    "        if i in PLOT_AT:\n",
    "            md('### Plotting after '+str(i)+' samples')\n",
    "            plt.plot(μs, σs, alpha=0.2)\n",
    "            plt.scatter(μs, σs, marker='.')\n",
    "            plt.title('All samples')\n",
    "            plt.show()\n",
    "\n",
    "            plt.scatter(μs[BURN:], σs[BURN:], marker='x')\n",
    "            plt.xlim(np.min(μs), np.max(μs))\n",
    "            plt.ylim(np.min(σs), np.max(σs))\n",
    "            plt.title('Samples after burn-in period')\n",
    "            plt.show()\n",
    "            \n",
    "            plt.scatter(μs[BURN:], σs[BURN:], marker='x')\n",
    "            plt.title('Samples after burn-in period (zoomed)')\n",
    "            plt.show()\n",
    "\n",
    "\n",
    "    print('Estimating the p(θ|X) as an histogram from samples (true posterior with lines)')\n",
    "    show_heatmap(μs[BURN:], σs[BURN:], bins=25)\n",
    "    plot_true_distribution()\n",
    "    plt.show()\n",
    "    return obj_dic(locals())\n",
    "\n",
    "gibbs = do_gibbs_uninformative_prior()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee4e1ae8-8449-4059-a05a-760eea0692e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "md('# METROPOLIS-HASTINGS EXAMPLE')\n",
    "\n",
    "def do_metropolis_hastings(prior=False, show=True, ITER=20000, INIT=(10,0.1)):\n",
    "    μ, σ = INIT ####### METROPOLIS-HASTINGS: initialize\n",
    "\n",
    "    μs = [μ]\n",
    "    σs = [σ]\n",
    "    rej_μs = []\n",
    "    rej_σs = []\n",
    "\n",
    "    # log likelihood function\n",
    "    lnLproportional = lambda μ, σ: -(X.shape[0]*np.log(σ) + np.sum((X - μ)**2)/2/σ**2) # up to log((2π)^½N)\n",
    "    lnLratio = lambda μn, σn, μ, σ: lnLproportional(μn, σn) - lnLproportional(μ, σ) # log(a/b) = log(a) - log(b)\n",
    "    # log prior function\n",
    "    # NB: non-conjugate prior, we have no contraint here\n",
    "    lnprior = lambda μ, σ: -(μ-5)**2/.2 + -(σ-3)**2/.2  # kind of gaussian prior around μ=5, σ=2, pretty tight, so we see that even with 100 observations the prior impacts significantly what is the optimal p(μ,σ|X)\n",
    "    \n",
    "    if prior == \"Funky\": # softly \"forbid\" a region to force the sample to get around it\n",
    "        # a improper prior that is almost 0 if σ≥2.5, and transitions to 1 at σ=1.5\n",
    "        # the 100 is the importance of the prior compared to the data term lnLproportional\n",
    "        lnprior = lambda μ, σ: 100*np.log(np.clip(2.5 - σ, 0.0001, 1))\n",
    "        print(\"Funky\")\n",
    "        # warning: we could create bad local optimum with the prior, which can cause a lot of rejected samples\n",
    "    \n",
    "    BURN = 200\n",
    "    PLOT_AT = [200, 400]\n",
    "    if not show:\n",
    "        PLOT_AT = [ITER-1]\n",
    "        \n",
    "    for i in range(ITER): ####### METROPOLIS-HASTINGS: loop\n",
    "        μσvar = .2\n",
    "        μnew, σnew = norm.rvs(μ, μσvar), norm.rvs(σ, μσvar) # using a proposal distribution that is a normal around the current parameters\n",
    "        \n",
    "        if prior is not False:\n",
    "            paccept = np.exp(\n",
    "                lnLproportional(μnew, σnew) - lnLproportional(μ, σ)\n",
    "                + lnprior(μnew, σnew) - lnprior(μ, σ) # prior\n",
    "            ) ####### METROPOLIS-HASTINGS: compute acceptance \"probability\" (might be > 1)\n",
    "        else:\n",
    "            #paccept = L(μnew, σnew, X) / L(μ, σ, X) # no prior, not numerically stable\n",
    "            #paccept = np.exp( lnLproportional(μnew, σnew) - lnLproportional(μ, σ)   )  # no prior\n",
    "            paccept = np.exp(lnLratio(μnew, σnew, μ, σ)) # no prior\n",
    "        \n",
    "        if np.random.uniform(0, 1) < paccept: ####### METROPOLIS-HASTINGS: draw the acceptance\n",
    "            μ, σ = μnew, σnew ####### METROPOLIS-HASTINGS: accepted\n",
    "        else:\n",
    "            rej_μs.append(μnew)\n",
    "            rej_σs.append(σnew)\n",
    "        μs.append(μ)\n",
    "        σs.append(σ)\n",
    "\n",
    "        if i in PLOT_AT:\n",
    "            plt.plot(μs, σs, alpha=0.6)\n",
    "            if show:\n",
    "                plt.scatter(μs, σs, marker='x')\n",
    "                plt.scatter(rej_μs, rej_σs, marker='x', c='r')\n",
    "                show_heatmap_contours(norm.rvs(μ, μσvar, 10000), norm.rvs(σ, μσvar, 10000))\n",
    "                plt.show()\n",
    "            if i > BURN:\n",
    "                plt.scatter(μs[BURN:], σs[BURN:], marker='x')\n",
    "                if show:\n",
    "                    plt.show()\n",
    "            \n",
    "    if show:\n",
    "        show_heatmap(μs[BURN:], σs[BURN:], bins=25)\n",
    "        plot_true_distribution()\n",
    "        plt.show()\n",
    "    md(\"Accepted \" + str(len(μs)-len(rej_μs)) + \" and rejected \" + str(len(rej_μs)))\n",
    "    return obj_dic(locals())\n",
    "\n",
    "\n",
    "md(\"## Showing accepted samples and rejected samples (1 chain)\")\n",
    "md(\"### No prior\")\n",
    "mh = do_metropolis_hastings()\n",
    "md(\"### Prior that biases the estimate\")\n",
    "mh = do_metropolis_hastings(True)\n",
    "md(\"### Prior that forbids σ > 2.5\")\n",
    "mh = do_metropolis_hastings(\"Funky\")\n",
    "\n",
    "\n",
    "md(\"## Draw a few chains\")\n",
    "for setup in [False, True, 'Funky']:\n",
    "    for i in range(10):\n",
    "        do_metropolis_hastings(setup, show=False, ITER=500, INIT=(norm.rvs(10, 2), np.abs(norm.rvs(1, 2))))\n",
    "    plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f15574bb-db64-4e2d-996b-3fa5fb9b21f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "md('# VARIATIONAL INFERENCE EXAMPLE')\n",
    "\n",
    "def do_variational_inference(PRIOR=(5,1 , 1,1.5**2), ITER=1000000):\n",
    "    µ0, λ0, α0, β0 = PRIOR ####### VARIATIONAL INFERENCE: prior\n",
    "    \n",
    "    ####### VARIATIONAL INFERENCE:  set inital values of the variational parameters using the prior\n",
    "    μN = µ0\n",
    "    σN = (β0/α0/λ0)**0.5\n",
    "    αN = α0\n",
    "    βN = β0\n",
    "\n",
    "    ####### VARIATIONAL INFERENCE: constants\n",
    "    Xsum = np.sum(X)\n",
    "    X2sum = np.sum(X**2)\n",
    "    #print(Xsum, X2sum, N)\n",
    "\n",
    "    history = []\n",
    "    history.append([μN,σN,αN,βN])\n",
    "\n",
    "    PLOT_AT = [ITER-1]\n",
    "    PLOT_PDF_AT = [0, 1, 2, 3, 4, ITER-1]\n",
    "    PLOT_BROAD =  [0, 1, 2, 3]\n",
    "    for i in range(ITER): ####### VARIATIONAL INFERENCE: loop\n",
    "        # actually it is all at the same time\n",
    "        αN,βN,μN,σN = (\n",
    "            α0 + (N+1)/2,\n",
    "            β0 + 0.5 * ( (λ0+N)*(σN**2+μN**2) - 2*(μ0*λ0+Xsum)*μN + X2sum + λ0*μ0**2 ),\n",
    "            (Xsum + λ0*μ0) / (N + λ0),\n",
    "            (βN/αN/(N+λ0))**0.5,\n",
    "        ) ####### VARIATIONAL INFERENCE: closed form deterministic update\n",
    "        history.append([μN,σN,αN,βN])\n",
    "\n",
    "    for i in sorted(list(set(PLOT_AT + PLOT_PDF_AT))):\n",
    "        μN,σN,αN,βN = history[i]\n",
    "        if i in PLOT_PDF_AT:\n",
    "            md('Current variational estimation q(θ) (heatmap) vs true posterior p(θ|X) (isolines) (and also without prior)')\n",
    "            linspaceμ = default_plot.linspaceμ\n",
    "            linspaceσ = default_plot.linspaceσ\n",
    "            if i in PLOT_BROAD:\n",
    "                linspaceμ = np.linspace(0, 6, 303)\n",
    "                linspaceσ = np.linspace(0.01, 2, 301)\n",
    "            pσ = invgamma.pdf(linspaceσ**2, αN, 0, βN)\n",
    "            pμ = norm.pdf(linspaceμ[:,None], μN, σN)\n",
    "            pμσ = pμ*pσ[None,:]\n",
    "\n",
    "            mm = default_plot.mm\n",
    "            plt.imshow(pμσ.T, extent=[*mm(linspaceμ), *mm(linspaceσ)], origin='lower', aspect='auto')\n",
    "            #plt.colorbar()\n",
    "            plot_true_distribution()\n",
    "            plot_true_distribution(prior=lambda μ,σ: invgamma.pdf(σ**2, α0, 0, β0) * norm.pdf(μ, μ0, σ/λ0**0.5))\n",
    "            plt.title(\"$q(\\\\mu, \\\\sigma)^{(color)}$ vs $p(\\\\mu, \\\\sigma | X)^{(isolines)}$, after \"+str(i+1)+\" iter.\")\n",
    "            plt.show()\n",
    " \n",
    "        if i in PLOT_AT:\n",
    "            md('Plotting the mean of the estimate accross iterations (we see a convergence)')\n",
    "            h = np.array(history)\n",
    "            μs = h[:,0]\n",
    "            σs = (h[:,3]/h[:,2])**0.5\n",
    "            plt.title(\"successive positions of the mean of $q(\\\\mu, \\\\sigma)$\")\n",
    "            plt.plot(μs, σs, alpha=0.6)\n",
    "            plt.scatter(μs, σs, marker='x')\n",
    "            plt.show()\n",
    "\n",
    "    return obj_dic(locals())\n",
    "\n",
    "md('## Not too bad prior and initialization')\n",
    "vi = do_variational_inference()\n",
    "md('## Poor prior and initialization')\n",
    "# 42 virtual points at value 5, \n",
    "vi = do_variational_inference(PRIOR=(5,42 , 1,1.5**2), ITER=10)\n",
    "md('## Fuzzy prior and initialization')\n",
    "# 5 points at 5\n",
    "vi = do_variational_inference(PRIOR=(5,5 , 1,1.5**2), ITER=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea4dce1a-9ad3-4b13-a81b-626edef6e28c",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb1b5dcf-cdd2-4ce5-ad0d-89b22a506c5a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4f2b6c6-3e76-45ef-a9cf-57217ce7cb0e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f112bc84-4684-432e-9642-34c29c83a80b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcfb415b-a795-4c60-a043-ede7943edcce",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65f5f131-d02b-4315-9913-392926334f33",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3569235-e870-465f-8d09-ba88acf1fda4",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5e12ec04-79b8-4a31-896b-2d9cee7e5a15",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e041c53f-c539-4062-9e49-05e71fdadd6a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
